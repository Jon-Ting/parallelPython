{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e3ba98c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-18T01:00:46.200224Z",
     "iopub.status.busy": "2024-04-18T01:00:46.198967Z",
     "iopub.status.idle": "2024-04-18T01:00:46.564603Z",
     "shell.execute_reply": "2024-04-18T01:00:46.563728Z",
     "shell.execute_reply.started": "2024-04-18T01:00:46.200161Z"
    }
   },
   "outputs": [],
   "source": [
    "import numba\n",
    "from numba import jit, int32, prange, vectorize, float64, cuda\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37f8f218",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-18T01:00:51.836408Z",
     "iopub.status.busy": "2024-04-18T01:00:51.835452Z",
     "iopub.status.idle": "2024-04-18T01:00:51.844813Z",
     "shell.execute_reply": "2024-04-18T01:00:51.843156Z",
     "shell.execute_reply.started": "2024-04-18T01:00:51.836350Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function that calculates the value of Pi using monte-carlo method\n",
    "# Minimal use of Numpy\n",
    "def native_python_monte_carlo(n):\n",
    "    within_circle = 0\n",
    "    \n",
    "    for i in range(int(n)):\n",
    "        x = np.random.random()\n",
    "        y = np.random.random()\n",
    "        \n",
    "        if x**2 + y**2 <= 1.0:\n",
    "            within_circle += 1\n",
    "        \n",
    "    return 4.0 * within_circle / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e775b265",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-18T01:00:53.701987Z",
     "iopub.status.busy": "2024-04-18T01:00:53.701246Z",
     "iopub.status.idle": "2024-04-18T01:01:04.751292Z",
     "shell.execute_reply": "2024-04-18T01:01:04.749929Z",
     "shell.execute_reply.started": "2024-04-18T01:00:53.701928Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.14163\n",
      "CPU times: user 11 s, sys: 10.6 ms, total: 11 s\n",
      "Wall time: 11 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(native_python_monte_carlo(10000000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3576c9e1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-18T01:01:08.536784Z",
     "iopub.status.busy": "2024-04-18T01:01:08.535997Z",
     "iopub.status.idle": "2024-04-18T01:01:08.545861Z",
     "shell.execute_reply": "2024-04-18T01:01:08.544509Z",
     "shell.execute_reply.started": "2024-04-18T01:01:08.536725Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function that calculates the value of Pi using monte-carlo method\n",
    "# Leveraged numpy as far as possible\n",
    "\n",
    "def numpy_python_monte_carlo(n):\n",
    "    within_circle = 0\n",
    "    \n",
    "    x = np.random.random(int(n))\n",
    "    y = np.random.random(int(n))\n",
    "        \n",
    "    within_circle = np.sum( (x**2 + y**2) <= 1.0 )\n",
    "        \n",
    "    return 4.0 * within_circle / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d07736e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-18T01:01:08.902178Z",
     "iopub.status.busy": "2024-04-18T01:01:08.901017Z",
     "iopub.status.idle": "2024-04-18T01:01:09.131076Z",
     "shell.execute_reply": "2024-04-18T01:01:09.130091Z",
     "shell.execute_reply.started": "2024-04-18T01:01:08.902118Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.1411872\n",
      "CPU times: user 168 ms, sys: 56 ms, total: 224 ms\n",
      "Wall time: 223 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(numpy_python_monte_carlo(10000000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288cd73c",
   "metadata": {
    "solution": "hidden",
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "### Use @jit on both functions and see what is the speedup achieved (ignore the first run)\n",
    "* Native python :\n",
    "* Numpy :\n",
    "\n",
    "* Natve python with Numba :\n",
    "* Numpy wuth Numba: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d3fb743",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-18T01:01:28.684186Z",
     "iopub.status.busy": "2024-04-18T01:01:28.683213Z",
     "iopub.status.idle": "2024-04-18T01:01:28.702265Z",
     "shell.execute_reply": "2024-04-18T01:01:28.701555Z",
     "shell.execute_reply.started": "2024-04-18T01:01:28.684127Z"
    },
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "@jit\n",
    "def native_python_monte_carlo(n):\n",
    "    within_circle = 0\n",
    "    \n",
    "    for i in range(int(n)):\n",
    "        x = np.random.random()\n",
    "        y = np.random.random()\n",
    "        \n",
    "        if x**2 + y**2 <= 1.0:\n",
    "           within_circle += 1\n",
    "        \n",
    "    return 4.0 * within_circle / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01ce80be-f473-49ae-ac40-d6d960c2eacd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-18T01:02:12.870653Z",
     "iopub.status.busy": "2024-04-18T01:02:12.869503Z",
     "iopub.status.idle": "2024-04-18T01:02:12.974078Z",
     "shell.execute_reply": "2024-04-18T01:02:12.973021Z",
     "shell.execute_reply.started": "2024-04-18T01:02:12.870591Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.140952\n",
      "CPU times: user 97.3 ms, sys: 0 ns, total: 97.3 ms\n",
      "Wall time: 96.8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(native_python_monte_carlo(10000000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4893dab8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-18T01:02:37.447472Z",
     "iopub.status.busy": "2024-04-18T01:02:37.446190Z",
     "iopub.status.idle": "2024-04-18T01:02:37.456691Z",
     "shell.execute_reply": "2024-04-18T01:02:37.455260Z",
     "shell.execute_reply.started": "2024-04-18T01:02:37.447407Z"
    },
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "@jit\n",
    "def numpy_python_monte_carlo(n):\n",
    "    within_circle = 0\n",
    "    \n",
    "    x = np.random.random(int(n))\n",
    "    y = np.random.random(int(n))\n",
    "        \n",
    "    within_circle = np.sum( (x**2 + y**2) <= 1.0 )\n",
    "        \n",
    "    return 4.0 * within_circle / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "16020834-df49-499b-81d3-5dbe8dfd3af2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-18T01:02:42.067505Z",
     "iopub.status.busy": "2024-04-18T01:02:42.066671Z",
     "iopub.status.idle": "2024-04-18T01:02:42.245917Z",
     "shell.execute_reply": "2024-04-18T01:02:42.244969Z",
     "shell.execute_reply.started": "2024-04-18T01:02:42.067447Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.1421672\n",
      "CPU times: user 124 ms, sys: 49.5 ms, total: 173 ms\n",
      "Wall time: 172 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(numpy_python_monte_carlo(10000000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c285d8f4",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "### What is the difference If we use eager compilation (ignore first run)?\n",
    "\n",
    "* Native python with Numba (Lazy):\n",
    "* Native python with Numba (Eager):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cbf6d03c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-18T01:02:51.342071Z",
     "iopub.status.busy": "2024-04-18T01:02:51.341102Z",
     "iopub.status.idle": "2024-04-18T01:02:51.455002Z",
     "shell.execute_reply": "2024-04-18T01:02:51.454081Z",
     "shell.execute_reply.started": "2024-04-18T01:02:51.342012Z"
    },
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "@jit(numba.float32(int32))\n",
    "def native_python_monte_carlo_eager(n):\n",
    "    within_circle = 0\n",
    "    \n",
    "    for i in range(int(n)):\n",
    "        x = np.random.random()\n",
    "        y = np.random.random()\n",
    "        \n",
    "        if x**2 + y**2 <= 1.0:\n",
    "            within_circle += 1\n",
    "        \n",
    "    return 4.0 * within_circle / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6525c155",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-18T01:03:20.188020Z",
     "iopub.status.busy": "2024-04-18T01:03:20.186890Z",
     "iopub.status.idle": "2024-04-18T01:03:20.292819Z",
     "shell.execute_reply": "2024-04-18T01:03:20.292374Z",
     "shell.execute_reply.started": "2024-04-18T01:03:20.187959Z"
    },
    "scrolled": true,
    "solution2": "hidden"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.142198324203491\n",
      "CPU times: user 101 ms, sys: 0 ns, total: 101 ms\n",
      "Wall time: 100 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(native_python_monte_carlo_eager(10000000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "79348754",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-18T01:03:32.054566Z",
     "iopub.status.busy": "2024-04-18T01:03:32.053343Z",
     "iopub.status.idle": "2024-04-18T01:03:32.064793Z",
     "shell.execute_reply": "2024-04-18T01:03:32.063180Z",
     "shell.execute_reply.started": "2024-04-18T01:03:32.054497Z"
    },
    "solution2": "hidden"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 368 µs, sys: 36 µs, total: 404 µs\n",
      "Wall time: 411 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "@jit\n",
    "def native_python_monte_carlo_lazy(n):\n",
    "    within_circle = 0\n",
    "    \n",
    "    for i in range(int(n)):\n",
    "        x = np.random.random()\n",
    "        y = np.random.random()\n",
    "        \n",
    "        if x**2 + y**2 <= 1.0:\n",
    "            within_circle += 1\n",
    "        \n",
    "    return 4.0 * within_circle / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0c0225cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-18T01:03:42.109229Z",
     "iopub.status.busy": "2024-04-18T01:03:42.108379Z",
     "iopub.status.idle": "2024-04-18T01:03:42.210843Z",
     "shell.execute_reply": "2024-04-18T01:03:42.210293Z",
     "shell.execute_reply.started": "2024-04-18T01:03:42.109169Z"
    },
    "scrolled": true,
    "solution2": "hidden"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.1421032\n",
      "CPU times: user 97.1 ms, sys: 0 ns, total: 97.1 ms\n",
      "Wall time: 96.7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(native_python_monte_carlo_lazy(10000000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd1c335",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "### What is the difference with and without automatic parallelization (ignore the first run)?\n",
    "hint: use *numba.prange* instead of *range*\n",
    "* With parallel:\n",
    "* Without parallel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "956e41cf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-18T01:04:00.994064Z",
     "iopub.status.busy": "2024-04-18T01:04:00.992869Z",
     "iopub.status.idle": "2024-04-18T01:04:01.002558Z",
     "shell.execute_reply": "2024-04-18T01:04:01.000977Z",
     "shell.execute_reply.started": "2024-04-18T01:04:00.993995Z"
    },
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "@jit(parallel=True)\n",
    "def native_python_monte_carlo_parallel(n):\n",
    "    within_circle = 0\n",
    "    \n",
    "    for i in numba.prange(int(n)):\n",
    "        x = np.random.random()\n",
    "        y = np.random.random()\n",
    "        \n",
    "        if x**2 + y**2 <= 1.0:\n",
    "            within_circle += 1\n",
    "        \n",
    "    return 4.0 * within_circle / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "87b2d663",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-18T01:04:07.508953Z",
     "iopub.status.busy": "2024-04-18T01:04:07.508114Z",
     "iopub.status.idle": "2024-04-18T01:04:07.570500Z",
     "shell.execute_reply": "2024-04-18T01:04:07.569998Z",
     "shell.execute_reply.started": "2024-04-18T01:04:07.508894Z"
    },
    "solution2": "hidden"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.1412404\n",
      "CPU times: user 108 ms, sys: 0 ns, total: 108 ms\n",
      "Wall time: 56 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(native_python_monte_carlo_parallel(10000000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87680c8b",
   "metadata": {},
   "source": [
    "### Offload the computation to a GPU\n",
    "* Assumptions:\n",
    "    * N <= 512\n",
    "* Hints:\n",
    "    * Launch one block with threads <= 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a8cf14e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-18T01:04:23.702614Z",
     "iopub.status.busy": "2024-04-18T01:04:23.701457Z",
     "iopub.status.idle": "2024-04-18T01:04:23.710935Z",
     "shell.execute_reply": "2024-04-18T01:04:23.709394Z",
     "shell.execute_reply.started": "2024-04-18T01:04:23.702547Z"
    },
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def native_python_monte_kernel(array_a, array_b, array_c):\n",
    "    # Thread id in a 1D block\n",
    "    tx = cuda.threadIdx.x\n",
    " \n",
    "    if tx < array_a.size:  # Check array boundaries\n",
    "        array_c[tx] = 1 if array_a[tx]**2 + array_b[tx]**2 <= 1.0  else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c6d52e7e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-18T01:04:35.125170Z",
     "iopub.status.busy": "2024-04-18T01:04:35.123968Z",
     "iopub.status.idle": "2024-04-18T01:04:35.132875Z",
     "shell.execute_reply": "2024-04-18T01:04:35.131225Z",
     "shell.execute_reply.started": "2024-04-18T01:04:35.125101Z"
    },
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "n = 10000\n",
    "blocks = 2\n",
    "threads = 256\n",
    "array_a = np.random.random(n)\n",
    "array_b = np.random.random(n)\n",
    "array_c = np.zeros(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c46482d5",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2024-04-18T01:04:38.430414Z",
     "iopub.status.busy": "2024-04-18T01:04:38.429267Z",
     "iopub.status.idle": "2024-04-18T01:04:39.210631Z",
     "shell.execute_reply": "2024-04-18T01:04:39.209384Z",
     "shell.execute_reply.started": "2024-04-18T01:04:38.430351Z"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/vp91/Training-Venv/parallelpython/lib/python3.10/site-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: Grid size 2 will likely result in GPU under-utilization due to low occupancy.\n",
      "  warn(NumbaPerformanceWarning(msg))\n"
     ]
    },
    {
     "ename": "CudaSupportError",
     "evalue": "Error at driver init: Call to cuInit results in CUDA_ERROR_NO_DEVICE (100)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCudaAPIError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m/scratch/vp91/Training-Venv/parallelpython/lib/python3.10/site-packages/numba/cuda/cudadrv/driver.py:254\u001b[0m, in \u001b[0;36mDriver.ensure_initialized\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    253\u001b[0m     _logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minit\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 254\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuInit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m CudaAPIError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/scratch/vp91/Training-Venv/parallelpython/lib/python3.10/site-packages/numba/cuda/cudadrv/driver.py:327\u001b[0m, in \u001b[0;36mDriver._ctypes_wrap_fn.<locals>.safe_cuda_api_call\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    326\u001b[0m retcode \u001b[38;5;241m=\u001b[39m libfn(\u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m--> 327\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_ctypes_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretcode\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/vp91/Training-Venv/parallelpython/lib/python3.10/site-packages/numba/cuda/cudadrv/driver.py:395\u001b[0m, in \u001b[0;36mDriver._check_ctypes_error\u001b[0;34m(self, fname, retcode)\u001b[0m\n\u001b[1;32m    394\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_fork()\n\u001b[0;32m--> 395\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m CudaAPIError(retcode, msg)\n",
      "\u001b[0;31mCudaAPIError\u001b[0m: [100] Call to cuInit results in CUDA_ERROR_NO_DEVICE",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mCudaSupportError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mnative_python_monte_kernel\u001b[49m\u001b[43m[\u001b[49m\u001b[43mblocks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreads\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray_a\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marray_b\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marray_c\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;241m4.0\u001b[39m \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39msum(array_c) \u001b[38;5;241m/\u001b[39m n)\n",
      "File \u001b[0;32m/scratch/vp91/Training-Venv/parallelpython/lib/python3.10/site-packages/numba/cuda/dispatcher.py:539\u001b[0m, in \u001b[0;36m_LaunchConfiguration.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    538\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs):\n\u001b[0;32m--> 539\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgriddim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblockdim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[43m                                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msharedmem\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/vp91/Training-Venv/parallelpython/lib/python3.10/site-packages/numba/cuda/dispatcher.py:681\u001b[0m, in \u001b[0;36mCUDADispatcher.call\u001b[0;34m(self, args, griddim, blockdim, stream, sharedmem)\u001b[0m\n\u001b[1;32m    679\u001b[0m     kernel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moverloads\u001b[38;5;241m.\u001b[39mvalues()))\n\u001b[1;32m    680\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 681\u001b[0m     kernel \u001b[38;5;241m=\u001b[39m \u001b[43m_dispatcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDispatcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    683\u001b[0m kernel\u001b[38;5;241m.\u001b[39mlaunch(args, griddim, blockdim, stream, sharedmem)\n",
      "File \u001b[0;32m/scratch/vp91/Training-Venv/parallelpython/lib/python3.10/site-packages/numba/cuda/dispatcher.py:689\u001b[0m, in \u001b[0;36mCUDADispatcher._compile_for_args\u001b[0;34m(self, *args, **kws)\u001b[0m\n\u001b[1;32m    687\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kws\n\u001b[1;32m    688\u001b[0m argtypes \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtypeof_pyval(a) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m args]\n\u001b[0;32m--> 689\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margtypes\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/vp91/Training-Venv/parallelpython/lib/python3.10/site-packages/numba/cuda/dispatcher.py:932\u001b[0m, in \u001b[0;36mCUDADispatcher.compile\u001b[0;34m(self, sig)\u001b[0m\n\u001b[1;32m    929\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_can_compile:\n\u001b[1;32m    930\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompilation disabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 932\u001b[0m kernel \u001b[38;5;241m=\u001b[39m \u001b[43m_Kernel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpy_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtargetoptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    933\u001b[0m \u001b[38;5;66;03m# We call bind to force codegen, so that there is a cubin to cache\u001b[39;00m\n\u001b[1;32m    934\u001b[0m kernel\u001b[38;5;241m.\u001b[39mbind()\n",
      "File \u001b[0;32m/scratch/vp91/Training-Venv/parallelpython/lib/python3.10/site-packages/numba/core/compiler_lock.py:35\u001b[0m, in \u001b[0;36m_CompilerLock.__call__.<locals>._acquire_compile_lock\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_acquire_compile_lock\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m---> 35\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/vp91/Training-Venv/parallelpython/lib/python3.10/site-packages/numba/cuda/dispatcher.py:82\u001b[0m, in \u001b[0;36m_Kernel.__init__\u001b[0;34m(self, py_func, argtypes, link, debug, lineinfo, inline, fastmath, extensions, max_registers, opt, device)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextensions \u001b[38;5;241m=\u001b[39m extensions \u001b[38;5;129;01mor\u001b[39;00m []\n\u001b[1;32m     77\u001b[0m nvvm_options \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfastmath\u001b[39m\u001b[38;5;124m'\u001b[39m: fastmath,\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mopt\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m3\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m opt \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     80\u001b[0m }\n\u001b[0;32m---> 82\u001b[0m cc \u001b[38;5;241m=\u001b[39m \u001b[43mget_current_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcompute_capability\n\u001b[1;32m     83\u001b[0m cres \u001b[38;5;241m=\u001b[39m compile_cuda(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpy_func, types\u001b[38;5;241m.\u001b[39mvoid, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margtypes,\n\u001b[1;32m     84\u001b[0m                     debug\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdebug,\n\u001b[1;32m     85\u001b[0m                     lineinfo\u001b[38;5;241m=\u001b[39mlineinfo,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     88\u001b[0m                     nvvm_options\u001b[38;5;241m=\u001b[39mnvvm_options,\n\u001b[1;32m     89\u001b[0m                     cc\u001b[38;5;241m=\u001b[39mcc)\n\u001b[1;32m     90\u001b[0m tgt_ctx \u001b[38;5;241m=\u001b[39m cres\u001b[38;5;241m.\u001b[39mtarget_context\n",
      "File \u001b[0;32m/scratch/vp91/Training-Venv/parallelpython/lib/python3.10/site-packages/numba/cuda/api.py:443\u001b[0m, in \u001b[0;36mget_current_device\u001b[0;34m()\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_current_device\u001b[39m():\n\u001b[1;32m    442\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGet current device associated with the current thread\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 443\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcurrent_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdevice\n",
      "File \u001b[0;32m/scratch/vp91/Training-Venv/parallelpython/lib/python3.10/site-packages/numba/cuda/cudadrv/devices.py:220\u001b[0m, in \u001b[0;36mget_context\u001b[0;34m(devnum)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_context\u001b[39m(devnum\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    217\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get the current device or use a device by device number, and\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;124;03m    return the CUDA context.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_runtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_or_create_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevnum\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/vp91/Training-Venv/parallelpython/lib/python3.10/site-packages/numba/cuda/cudadrv/devices.py:138\u001b[0m, in \u001b[0;36m_Runtime.get_or_create_context\u001b[0;34m(self, devnum)\u001b[0m\n\u001b[1;32m    136\u001b[0m attached_ctx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_attached_context()\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attached_ctx \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 138\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_or_create_context_uncached\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevnum\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m attached_ctx\n",
      "File \u001b[0;32m/scratch/vp91/Training-Venv/parallelpython/lib/python3.10/site-packages/numba/cuda/cudadrv/devices.py:153\u001b[0m, in \u001b[0;36m_Runtime._get_or_create_context_uncached\u001b[0;34m(self, devnum)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"See also ``get_or_create_context(devnum)``.\u001b[39;00m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;124;03mThis version does not read the cache.\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;66;03m# Try to get the active context in the CUDA stack or\u001b[39;00m\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;66;03m# activate GPU-0 with the primary context\u001b[39;00m\n\u001b[0;32m--> 153\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m driver\u001b[38;5;241m.\u001b[39mget_active_context() \u001b[38;5;28;01mas\u001b[39;00m ac:\n\u001b[1;32m    154\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ac:\n\u001b[1;32m    155\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_activate_context_for(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m/scratch/vp91/Training-Venv/parallelpython/lib/python3.10/site-packages/numba/cuda/cudadrv/driver.py:495\u001b[0m, in \u001b[0;36m_ActiveContext.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    493\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    494\u001b[0m     hctx \u001b[38;5;241m=\u001b[39m drvapi\u001b[38;5;241m.\u001b[39mcu_context(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 495\u001b[0m     \u001b[43mdriver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuCtxGetCurrent\u001b[49m(byref(hctx))\n\u001b[1;32m    496\u001b[0m     hctx \u001b[38;5;241m=\u001b[39m hctx \u001b[38;5;28;01mif\u001b[39;00m hctx\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hctx \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/scratch/vp91/Training-Venv/parallelpython/lib/python3.10/site-packages/numba/cuda/cudadrv/driver.py:292\u001b[0m, in \u001b[0;36mDriver.__getattr__\u001b[0;34m(self, fname)\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, fname):\n\u001b[1;32m    291\u001b[0m     \u001b[38;5;66;03m# First request of a driver API function\u001b[39;00m\n\u001b[0;32m--> 292\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mensure_initialized\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    294\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minitialization_error \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    295\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m CudaSupportError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError at driver init: \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m    296\u001b[0m                                \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minitialization_error)\n",
      "File \u001b[0;32m/scratch/vp91/Training-Venv/parallelpython/lib/python3.10/site-packages/numba/cuda/cudadrv/driver.py:258\u001b[0m, in \u001b[0;36mDriver.ensure_initialized\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    256\u001b[0m     description \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;241m.\u001b[39mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;241m.\u001b[39mcode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    257\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minitialization_error \u001b[38;5;241m=\u001b[39m description\n\u001b[0;32m--> 258\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CudaSupportError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError at driver init: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdescription\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpid \u001b[38;5;241m=\u001b[39m _getpid()\n",
      "\u001b[0;31mCudaSupportError\u001b[0m: Error at driver init: Call to cuInit results in CUDA_ERROR_NO_DEVICE (100)"
     ]
    }
   ],
   "source": [
    "native_python_monte_kernel[blocks, threads](array_a, array_b, array_c)\n",
    "print(4.0 * np.sum(array_c) / n)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
